# Cache Line Utilization Analysis

## Quick Reference

| Structure | Useful Bytes | Cache Line Utilization | Notes |
|-----------|--------------|------------------------|-------|
| `cake_task_ctx` | 16 / 64 | **25%** (4 per cache line = **100%**) | Per-task context (hot path) |
| `cake_cpu_status` | 1 / 64 | **1.56%** | ‚ö†Ô∏è Intentional padding (prevents false sharing) |
| `cake_cooldown_status` | 8 / 64 | **12.5%** | ‚ö†Ô∏è Intentional padding (prevents false sharing) |
| `idle_mask` | 8 / 64 | **12.5%** | ‚ö†Ô∏è Intentional padding (prevents false sharing) |
| `victim_mask` | 8 / 64 | **12.5%** | ‚ö†Ô∏è Intentional padding (prevents false sharing) |
| `cake_stats` | 416 / 448 | **92.9%** | Per-CPU statistics (conditional) |
| `wait_budget[8]` | 64 / 64 | **100%** | ‚úÖ Perfect utilization |
| `starvation_threshold[8]` | 64 / 64 | **100%** | ‚úÖ Perfect utilization |
| `tier_multiplier[8]` | 32 / 64 | **50%** | Read-only array |

**Key Insight**: Structures with low utilization (1.56-12.5%) are **intentionally padded** to prevent false sharing. This is a performance optimization, not wasted space.

---

## Overview
This document analyzes cache line utilization across all data structures in `scx_cake`, identifying how many bytes out of each 64-byte cache line contain useful information.

**Cache Line Size**: 64 bytes (standard on x86-64)

---

## 1. Per-Task Context (`cake_task_ctx`)

**Location**: `src/bpf/intf.h:62-72`

```c
struct cake_task_ctx {
    u32 next_slice;        /* 4 bytes: Reduced from u64 */
    u32 last_run_at;       /* 4 bytes */
    u32 last_wake_ts;      /* 4 bytes */
    u32 packed_info;       /* 4 bytes */
    u32 runtime_deficit;   /* 4 bytes: Packed (deficit_us:16, avg_runtime_us:16) */
};
```

**Size Calculation**:
- Total: 4 + 4 + 4 + 4 + 4 = **16 bytes**
- Alignment: Natural alignment (no padding needed)

**Cache Line Utilization**:
- **Useful bytes per context**: 16 / 64 = **25%**
- **Contexts per cache line**: 64 / 16 = **4 contexts**
- **Total utilization**: 4 √ó 16 = **100%** ‚úÖ
- **Wasted bytes**: 0 bytes per cache line

**Analysis**:
- ‚úÖ **Perfect**: Exactly 4 contexts fit per 64-byte cache line
- ‚úÖ **Optimal locality**: All 4 contexts loaded together, maximizing cache efficiency
- ‚úÖ **Reduced memory bandwidth**: Fewer cache line fetches needed
- ‚úÖ **Better L1 cache utilization**: More contexts fit in L1 cache

**Optimizations Applied**:
1. **`next_slice`**: Reduced from u64 to u32 (max 5.2ms fits in u32, saves 4 bytes)
2. **Runtime fields**: Packed `deficit_us` and `avg_runtime_us` into single u32 (better alignment)

**Hot Path Impact**:
- This structure is accessed on every task wakeup (`cake_enqueue`)
- L1 cache hit: ~3-4 cycles
- **Improved**: 4 contexts per cache line means better prefetching and locality
- **Throughput**: More contexts can be processed with fewer cache misses

---

## 2. CPU Status Scoreboard (`cake_cpu_status`)

**Location**: `src/bpf/cake.bpf.c:74-77`

```c
struct cake_cpu_status {
    u8 tier;               /* 1 byte */
    u8 __pad[63];          /* 63 bytes padding */
};
```

**Size Calculation**:
- Total: **64 bytes** (exactly one cache line)
- Useful: **1 byte** (tier)
- Padding: **63 bytes** (intentional)

**Cache Line Utilization**:
- **Useful bytes**: 1 / 64 = **1.56%**
- **Wasted bytes**: 63 bytes (intentional padding)

**Analysis**:
- ‚úÖ **Excellent design**: Intentional padding prevents false sharing
- **Purpose**: Each CPU updates its own tier status atomically
- **Without padding**: Multiple CPUs would share cache lines ‚Üí cache line ping-pong (50-100 cycles per update)
- **With padding**: Each CPU has its own cache line ‚Üí no false sharing

**False Sharing Prevention**:
- On 8-core system: 8 CPUs √ó 64 bytes = 512 bytes total
- Each CPU's status is isolated in its own cache line
- Updates are lock-free and don't interfere with each other

**Hot Path Impact**:
- Updated in `cake_running` when tier changes
- Read in `cake_select_cpu` for preemption decisions
- O(1) array lookup: ~5 cycles

---

## 3. Preemption Cooldown (`cake_cooldown_status`)

**Location**: `src/bpf/cake.bpf.c:216-219`

```c
struct cake_cooldown_status {
    u64 last_preempt_ts;   /* 8 bytes */
    u8 __pad[56];          /* 56 bytes padding */
};
```

**Size Calculation**:
- Total: **64 bytes** (exactly one cache line)
- Useful: **8 bytes** (timestamp)
- Padding: **56 bytes** (intentional)

**Cache Line Utilization**:
- **Useful bytes**: 8 / 64 = **12.5%**
- **Wasted bytes**: 56 bytes (intentional padding)

**Analysis**:
- ‚úÖ **Excellent design**: Same false-sharing prevention as `cake_cpu_status`
- **Purpose**: Rate-limit preemption injection (prevents CPU thrashing)
- **Update frequency**: Only when preemption is injected (~rare)

**Hot Path Impact**:
- Checked in `cake_select_cpu` before preemption injection
- Updated only when preemption occurs (low frequency)
- Minimal impact on hot path

---

## 4. Global Bitmasks (Idle & Victim)

**Location**: `src/bpf/cake.bpf.c:48-65`

```c
u64 idle_mask SEC(".bss");        /* 8 bytes */
u64 __mask_pad[7] SEC(".bss");   /* 56 bytes padding */
u64 victim_mask SEC(".bss");     /* 8 bytes */
```

**Size Calculation**:
- `idle_mask`: 8 bytes
- `__mask_pad`: 7 √ó 8 = 56 bytes
- `victim_mask`: 8 bytes
- **Total**: 72 bytes across 2 cache lines

**Cache Line Utilization**:

**Cache Line 1** (idle_mask):
- **Useful bytes**: 8 / 64 = **12.5%**
- **Wasted bytes**: 56 bytes (intentional padding)

**Cache Line 2** (victim_mask):
- **Useful bytes**: 8 / 64 = **12.5%**
- **Wasted bytes**: 56 bytes (intentional padding)

**Analysis**:
- ‚úÖ **Critical optimization**: Prevents false sharing between two frequently-updated atomics
- **Without padding**: Both masks on same cache line ‚Üí cache line ping-pong on every idle transition
- **With padding**: Each mask isolated ‚Üí no interference

**Update Frequency**:
- `idle_mask`: Updated on every CPU idle/busy transition (~50,000/sec on 9800X3D)
- `victim_mask`: Updated lazily when tier changes (much lower frequency)

**Hot Path Impact**:
- `idle_mask` read in `cake_select_cpu` (hot path) - O(1) TZCNT instruction
- `victim_mask` read in `cake_select_cpu` for preemption injection
- Both are global variables (no map lookup overhead)

---

## 5. Statistics Structure (`cake_stats`)

**Location**: `src/bpf/intf.h:88-105`

```c
struct cake_stats {
    u64 nr_new_flow_dispatches;              /* 8 bytes */
    u64 nr_old_flow_dispatches;               /* 8 bytes */
    u64 nr_tier_dispatches[7];               /* 56 bytes (7 √ó 8) */
    u64 nr_sparse_promotions;                 /* 8 bytes */
    u64 nr_sparse_demotions;                  /* 8 bytes */
    u64 nr_wait_demotions;                    /* 8 bytes */
    u64 nr_wait_demotions_tier[7];           /* 56 bytes (7 √ó 8) */
    u64 nr_starvation_preempts_tier[7];      /* 56 bytes (7 √ó 8) */
    u64 total_wait_ns;                        /* 8 bytes */
    u64 nr_waits;                             /* 8 bytes */
    u64 max_wait_ns;                          /* 8 bytes */
    u64 total_wait_ns_tier[7];                /* 56 bytes (7 √ó 8) */
    u64 nr_waits_tier[7];                     /* 56 bytes (7 √ó 8) */
    u64 max_wait_ns_tier[7];                  /* 56 bytes (7 √ó 8) */
    u64 nr_input_preempts;                    /* 8 bytes */
};
```

**Size Calculation**:
- Scalar fields: 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 + 8 = 80 bytes
- Arrays: (7 √ó 8) √ó 6 = 336 bytes
- **Total**: 416 bytes

**Cache Line Utilization**:
- **Cache lines needed**: 416 / 64 = 6.5 (7 cache lines)
- **Useful bytes**: 416 / (7 √ó 64) = **92.9%**
- **Wasted in last cache line**: 7 √ó 64 - 416 = 32 bytes

**Analysis**:
- ‚úÖ **Excellent utilization**: 92.9% of cache lines contain useful data
- **Purpose**: Per-CPU statistics (only collected when `--verbose` is enabled)
- **Storage**: `BPF_MAP_TYPE_PERCPU_ARRAY` - each CPU has its own copy
- **Access pattern**: Write-heavy (increments), read by userspace TUI

**Hot Path Impact**:
- **Conditional**: Only updated when `enable_stats == true`
- **Cost**: Map lookup + increment (~20-30 cycles when enabled)
- **Optimization**: Stats disabled by default (zero overhead in production)

---

## 6. Static Arrays (Tier Multipliers, Wait Budgets, Starvation Thresholds)

**Location**: `src/bpf/cake.bpf.c:126-191`

### Tier Multipliers
```c
static const u32 tier_multiplier[8] = { ... };
```
- **Size**: 8 √ó 4 = **32 bytes**
- **Cache line utilization**: 32 / 64 = **50%**
- **Access**: Read-only, hot path (`update_task_tier`)

### Wait Budgets
```c
static const u64 wait_budget[8] = { ... };
```
- **Size**: 8 √ó 8 = **64 bytes** (exactly one cache line)
- **Cache line utilization**: **100%**
- **Access**: Read in `cake_running` (wait budget check)

### Starvation Thresholds
```c
static const u64 starvation_threshold[8] = { ... };
```
- **Size**: 8 √ó 8 = **64 bytes** (exactly one cache line)
- **Cache line utilization**: **100%**
- **Access**: Read in `cake_tick` (starvation check)

**Analysis**:
- ‚úÖ **Perfect utilization**: Wait budgets and starvation thresholds use 100% of cache line
- ‚úÖ **Read-only**: No false sharing concerns
- ‚úÖ **Hot path**: Frequently accessed, benefits from L1 cache

---

## Summary Table

| Data Structure | Size | Useful Bytes | Cache Line Utilization | Purpose |
|---------------|------|--------------|------------------------|---------|
| `cake_task_ctx` | 24 B | 24 B | **37.5%** | Per-task context (hot path) |
| `cake_cpu_status` | 64 B | 1 B | **1.56%** | CPU tier scoreboard (padded to prevent false sharing) |
| `cake_cooldown_status` | 64 B | 8 B | **12.5%** | Preemption cooldown (padded to prevent false sharing) |
| `idle_mask` | 8 B | 8 B | **12.5%** | Idle CPU bitmask (padded to prevent false sharing) |
| `victim_mask` | 8 B | 8 B | **12.5%** | Victim CPU bitmask (padded to prevent false sharing) |
| `cake_stats` | 416 B | 416 B | **92.9%** | Per-CPU statistics (conditional) |
| `wait_budget[8]` | 64 B | 64 B | **100%** | Wait budget thresholds |
| `starvation_threshold[8]` | 64 B | 64 B | **100%** | Starvation limits |
| `tier_multiplier[8]` | 32 B | 32 B | **50%** | Tier slice multipliers |

---

## Key Insights

### 1. Intentional Low Utilization (False Sharing Prevention)
The structures with **1.56%** and **12.5%** utilization (`cake_cpu_status`, `cake_cooldown_status`, bitmasks) are **intentionally padded** to prevent false sharing:

- **Without padding**: Multiple CPUs updating adjacent variables ‚Üí cache line ping-pong ‚Üí 50-100 cycles per update
- **With padding**: Each CPU's data isolated ‚Üí no interference ‚Üí atomic updates are fast

**Trade-off**: "Wasted" space is actually a performance optimization that saves hundreds of cycles per second.

### 2. Hot Path Structures
- `cake_task_ctx` (37.5%): Accessed on every task wakeup. Compact size allows 2-3 contexts per cache line.
- Static arrays (50-100%): Read-only, perfect cache line utilization, frequently accessed.

### 3. Statistics Structure
- `cake_stats` (92.9%): Excellent utilization, but only enabled when `--verbose` is used.
- Per-CPU storage prevents contention.

---

## Recommendations

### ‚úÖ Already Optimized
1. **False sharing prevention**: All frequently-updated atomics are properly padded
2. **Read-only arrays**: Perfect cache line utilization (100%)
3. **Task context**: Reasonable size, fits multiple contexts per cache line

### üí° Potential Improvements (if needed)

1. **`cake_task_ctx` packing** (if more fields needed):
   - Could pack `deficit_us` and `avg_runtime_us` into a single `u32` if ranges allow
   - Current 24-byte size is already good

2. **Statistics structure** (if size becomes concern):
   - Could split into hot-path stats (small) and cold-path stats (large)
   - Currently only enabled with `--verbose`, so not critical

3. **Tier multiplier array**:
   - Could combine with other small arrays to fill cache line
   - Current 50% utilization is acceptable for read-only data

---

## Conclusion

**Overall Cache Line Efficiency**: The scheduler demonstrates excellent cache line awareness:

- **Intentional low utilization** (1.56-12.5%) for atomics prevents false sharing (critical optimization)
- **Perfect utilization** (100%) for read-only hot-path arrays
- **Good utilization** (37.5-92.9%) for data structures

The "wasted" bytes in padded structures are a **performance feature**, not a bug. They prevent cache line contention that would cost 50-100 cycles per update on multi-core systems.

**Average useful bytes per cache line**: Varies by structure, but the design prioritizes **performance over density** for hot-path data structures.

